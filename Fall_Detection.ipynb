{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wTpm3nIMlVv"
   },
   "source": [
    "#Combining CNN and LSTM (RNN) to Detect Falls From Video Clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kf6VzYMQQZqO"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "# drive.flush_and_unmount()  # Unmount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15973,
     "status": "ok",
     "timestamp": 1739294391141,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "mYibZ8ZKLOmv",
    "outputId": "02fb8511-46d8-4809-b7c9-f95d6389ccd7"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDEqLru9L6IN"
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, cv2, random, shutil\n",
    "os.chdir(\"drive/My Drive/Fall_Detection_Playground_Senior_Prjct_II\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1739294394952,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "Hld3LUhtMBdG",
    "outputId": "8dbcd41b-8b56-40a2-de02-b4d7c61e6317"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYsgiynbOYHS"
   },
   "source": [
    "##Checking Class Imbalances and Video Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1739293702406,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "HtNLz1iAM1xf",
    "outputId": "d1cf0499-bbf4-4ac4-ea82-f859da2aa05f"
   },
   "outputs": [],
   "source": [
    "!ls videos_cropped/fall | wc -l\n",
    "!ls videos_cropped/no_fall | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739293703904,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "2CCVFJa3PHIi",
    "outputId": "60348d08-339f-4558-c7c6-b100f1244ab2"
   },
   "outputs": [],
   "source": [
    "fall_ratio = 30/70\n",
    "no_fall_ratio = 40/70\n",
    "print(\"fall class ratio:\", round(30/70, 2))\n",
    "print(\"no_fall class ratio:\", round(40/70, 2))\n",
    "print(\"overall_imbalance_ratio: 1:\" + str(round(no_fall_ratio/fall_ratio, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739293705981,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "i2Wa8NTTNgOq",
    "outputId": "d4879635-7042-42e3-cf42-1489d4ee5811"
   },
   "outputs": [],
   "source": [
    "fall_folder = \"videos_cropped/fall/\"\n",
    "no_fall_folder = \"videos_cropped/no_fall/\"\n",
    "\n",
    "fall_videos = os.listdir(fall_folder)\n",
    "no_fall_videos= os.listdir(no_fall_folder)\n",
    "\n",
    "for idx in range(len(fall_videos)):\n",
    "  fall_videos[idx] = fall_folder + fall_videos[idx]\n",
    "\n",
    "for idx in range(len(no_fall_videos)):\n",
    "  no_fall_videos[idx] = no_fall_folder + no_fall_videos[idx]\n",
    "\n",
    "print(\"Fall videos:\", fall_videos)\n",
    "print(\"No-Fall videos:\", no_fall_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1320,
     "status": "ok",
     "timestamp": 1739293709425,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "z4Dgy5wvRqHm",
    "outputId": "0df2295a-59ac-4d6f-d5ee-b9ca5af296a9"
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BPXjFGkQENl"
   },
   "outputs": [],
   "source": [
    "def with_opencv(filename):\n",
    "    video = cv2.VideoCapture(filename)\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error opening video file: {filename}\")\n",
    "        return None\n",
    "\n",
    "    # Get the frame rate\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Get the total number of frames\n",
    "    frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    # Calculate duration in seconds\n",
    "    duration = frame_count / fps if fps > 0 else 0\n",
    "\n",
    "    video.release()\n",
    "    return duration, fps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMoM3SclTK34"
   },
   "outputs": [],
   "source": [
    "def video_analysis(fall_videos, no_fall_videos):\n",
    "\n",
    "  min_fall = float('inf')\n",
    "  max_fall = 0\n",
    "  total_fall = 0\n",
    "  fall_fps = set()\n",
    "\n",
    "  min_no_fall = float('inf')\n",
    "  max_no_fall = 0\n",
    "  total_no_fall = 0\n",
    "  no_fall_fps = set()\n",
    "\n",
    "  for video in fall_videos:\n",
    "    if os.path.exists(video):\n",
    "      vid_info = with_opencv(video)\n",
    "      vid_length = vid_info[0]\n",
    "      fall_fps.add(vid_info[1])\n",
    "      min_fall = min(min_fall, vid_length)\n",
    "      max_fall = max(max_fall, vid_length)\n",
    "      total_fall += vid_length\n",
    "    else:\n",
    "      print(f\"File Not Found: {video}\")\n",
    "\n",
    "  for video in no_fall_videos:\n",
    "    if os.path.exists(video):\n",
    "      vid_info = with_opencv(video)\n",
    "      vid_length = vid_info[0]\n",
    "      no_fall_fps.add(vid_info[1])\n",
    "      min_no_fall = min(min_no_fall, vid_length)\n",
    "      max_no_fall = max(max_no_fall, vid_length)\n",
    "      total_no_fall += vid_length\n",
    "    else:\n",
    "      print(f\"File Not Found: {video}\")\n",
    "\n",
    "  print(\"Shortest fall video length:\",  round(min_fall, 2))\n",
    "  print(\"Longest fall video length:\",  round(max_fall, 2))\n",
    "  print(\"Average fall video length:\",  round(total_fall/len(fall_videos), 2))\n",
    "  print(\"Fps Consistency :\", len(fall_fps)==1, \",\", list(fall_fps)[0])\n",
    "  print(\"\\n\")\n",
    "  print(\"Shortest no_fall video length:\",  round(min_no_fall, 2))\n",
    "  print(\"Longest no_fall video length:\",  round(max_no_fall, 2))\n",
    "  print(\"Average no_fall video length:\",  round(total_no_fall/len(no_fall_videos), 2))\n",
    "  print(\"Fps Consistency :\", len(no_fall_fps)==1, \",\", list(no_fall_fps)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1739293716746,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "zImG4TQ-z50h",
    "outputId": "70880245-0627-4132-ea2e-3cffcdb5c756"
   },
   "outputs": [],
   "source": [
    "video_analysis(fall_videos, no_fall_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdMV_m3vNpOb"
   },
   "source": [
    "##Train-Test-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739293718556,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "_58GOmotNtLU",
    "outputId": "acfa9c01-9bc6-4627-af3b-bc644a858b9c"
   },
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "random.seed(1)\n",
    "\n",
    "# Function to split videos into the three sets\n",
    "def split_videos(video_list, train_size, val_size, test_size):\n",
    "    random.shuffle(video_list)  # Shuffle the videos to ensure random split\n",
    "\n",
    "    # Split based on the sizes\n",
    "    train = video_list[:train_size]\n",
    "    val = video_list[train_size:train_size + val_size]\n",
    "    test = video_list[train_size + val_size:train_size + val_size + test_size]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "# Get the video filenames\n",
    "fall_videos = os.listdir(fall_folder)\n",
    "no_fall_videos = os.listdir(no_fall_folder)\n",
    "\n",
    "# Split the videos (for fall and no_fall separately)\n",
    "fall_train, fall_val, fall_test = split_videos(fall_videos, train_size=15, val_size=5, test_size=10)\n",
    "no_fall_train, no_fall_val, no_fall_test = split_videos(no_fall_videos, train_size=20, val_size=5, test_size=10)\n",
    "\n",
    "# Check the splits\n",
    "print(\"Fall train:\", len(fall_train))\n",
    "print(\"Fall val:\", len(fall_val))\n",
    "print(\"Fall test:\", len(fall_test))\n",
    "\n",
    "print(\"No-fall train:\", len(no_fall_train))\n",
    "print(\"No-fall val:\", len(no_fall_val))\n",
    "print(\"No-fall test:\", len(no_fall_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1739293723372,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "G2OSSJC4PqfO",
    "outputId": "0d000c65-2325-45ba-ee8d-e5d4451c3a85"
   },
   "outputs": [],
   "source": [
    "# New base folder for final splits\n",
    "base_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/final_splits/\"\n",
    "\n",
    "# Create directories for the splits\n",
    "train_fall_folder = os.path.join(base_folder, \"train/fall/\")\n",
    "val_fall_folder = os.path.join(base_folder, \"val/fall/\")\n",
    "test_fall_folder = os.path.join(base_folder, \"test/fall/\")\n",
    "\n",
    "train_no_fall_folder = os.path.join(base_folder, \"train/no_fall/\")\n",
    "val_no_fall_folder = os.path.join(base_folder, \"val/no_fall/\")\n",
    "test_no_fall_folder = os.path.join(base_folder, \"test/no_fall/\")\n",
    "\n",
    "# Create the folders if they don't exist\n",
    "os.makedirs(train_fall_folder, exist_ok=True)\n",
    "os.makedirs(val_fall_folder, exist_ok=True)\n",
    "os.makedirs(test_fall_folder, exist_ok=True)\n",
    "\n",
    "os.makedirs(train_no_fall_folder, exist_ok=True)\n",
    "os.makedirs(val_no_fall_folder, exist_ok=True)\n",
    "os.makedirs(test_no_fall_folder, exist_ok=True)\n",
    "\n",
    "print(\"New folder structure created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32969,
     "status": "ok",
     "timestamp": 1739293759858,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "UWFWyeXjQZwm",
    "outputId": "bf5f4da0-056d-4152-d3c0-e016947c38eb"
   },
   "outputs": [],
   "source": [
    "# Finally, move the videos into their appropriate folders\n",
    "def move_files(video_list, src_folder, dst_folder):\n",
    "    for video in video_list:\n",
    "        shutil.copy(os.path.join(src_folder, video), os.path.join(dst_folder, video))\n",
    "\n",
    "# Move the fall videos\n",
    "move_files(fall_train, fall_folder, train_fall_folder)\n",
    "move_files(fall_val, fall_folder, val_fall_folder)\n",
    "move_files(fall_test, fall_folder, test_fall_folder)\n",
    "\n",
    "# Move the no_fall videos\n",
    "move_files(no_fall_train, no_fall_folder, train_no_fall_folder)\n",
    "move_files(no_fall_val, no_fall_folder, val_no_fall_folder)\n",
    "move_files(no_fall_test, no_fall_folder, test_no_fall_folder)\n",
    "\n",
    "print(\"Videos have been moved to the respective folders!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739293766857,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "pNXlE0AnRHb3",
    "outputId": "fffd380f-e57e-49bb-f3fb-13cd88acb484"
   },
   "outputs": [],
   "source": [
    "# Verifying the split\n",
    "print(\"Train Fall Videos:\", len(os.listdir(train_fall_folder)))\n",
    "print(\"Val Fall Videos:\", len(os.listdir(val_fall_folder)))\n",
    "print(\"Test Fall Videos:\", len(os.listdir(test_fall_folder)))\n",
    "\n",
    "print(\"Train No-Fall Videos:\", len(os.listdir(train_no_fall_folder)))\n",
    "print(\"Val No-Fall Videos:\", len(os.listdir(val_no_fall_folder)))\n",
    "print(\"Test No-Fall Videos:\", len(os.listdir(test_no_fall_folder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXa83wRkQIsv"
   },
   "source": [
    "##Augmenting Videos & Balancing the Two Classes\n",
    "\n",
    "#####We only have 15 and 20 videos for fall and no_fall classes in the training set, all of them being filmed in very similar environments with similar lighting, brightness, etc. Thus, we will be creating additional video clips for both of them with augmentations. We are also ensuring that both classes have the similar number of clips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YR585sqZOBMn"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1322,
     "status": "ok",
     "timestamp": 1739293771421,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "REWGs78HRqAL",
    "outputId": "d15c2396-5c96-4692-9bb6-19c1fa16d9bd"
   },
   "outputs": [],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYr1BrNnSfmq"
   },
   "outputs": [],
   "source": [
    "def rotate_video(video_frames, angle):\n",
    "    \"\"\"\n",
    "    Rotates the entire video by a fixed angle.\n",
    "    \"\"\"\n",
    "    rows, cols, _ = video_frames[0].shape\n",
    "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "    rotated_frames = [cv2.warpAffine(frame, M, (cols, rows)) for frame in video_frames]\n",
    "    return rotated_frames\n",
    "\n",
    "def flip_video(video_frames):\n",
    "    \"\"\"\n",
    "    Flips the entire video horizontally.\n",
    "    \"\"\"\n",
    "    return [cv2.flip(frame, 1) for frame in video_frames]  # Horizontal flip\n",
    "\n",
    "def adjust_brightness(video_frames):\n",
    "    factor = random.uniform(0.2, 2.0)  # Broader range to change brightness\n",
    "    adjusted_frames = [cv2.convertScaleAbs(frame, alpha=factor, beta=0) for frame in video_frames]\n",
    "    return adjusted_frames\n",
    "\n",
    "def adjust_contrast(video_frames):\n",
    "    factor = random.uniform(0.5, 3.0)  # Increase contrast range for stronger effect\n",
    "    adjusted_frames = [cv2.convertScaleAbs(frame, alpha=factor, beta=0) for frame in video_frames]\n",
    "    return adjusted_frames\n",
    "\n",
    "def adjust_saturation(video_frames):\n",
    "    adjusted_frames = []\n",
    "    saturation_factor = random.uniform(0.2, 2.0)  # Wider range for saturation\n",
    "    for frame in video_frames:\n",
    "        hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        hsv_frame[..., 1] = hsv_frame[..., 1] * saturation_factor\n",
    "        hsv_frame[..., 1] = np.clip(hsv_frame[..., 1], 0, 255)\n",
    "        adjusted_frame = cv2.cvtColor(hsv_frame, cv2.COLOR_HSV2BGR)\n",
    "        adjusted_frames.append(adjusted_frame)\n",
    "    return adjusted_frames\n",
    "\n",
    "def change_speed(video_frames, speed_factor=1.0):\n",
    "    total_frames = len(video_frames)\n",
    "    new_frame_count = int(total_frames * speed_factor)\n",
    "\n",
    "    if speed_factor < 0.5:\n",
    "        # Slowing down dramatically (take every nth frame)\n",
    "        step = max(1, int(1 / speed_factor))\n",
    "        new_frames = [video_frames[i] for i in range(0, total_frames, step)][:new_frame_count]\n",
    "    elif speed_factor > 1.5:\n",
    "        # Speeding up dramatically (sample frames more aggressively)\n",
    "        indices = np.linspace(0, total_frames - 1, new_frame_count, dtype=int)\n",
    "        new_frames = [video_frames[i] for i in indices]\n",
    "    else:\n",
    "        new_frames = video_frames\n",
    "\n",
    "    return new_frames\n",
    "\n",
    "def adjust_hue(video_frames):\n",
    "    \"\"\"\n",
    "    Adjusts the hue of the entire video by a random factor.\n",
    "    \"\"\"\n",
    "    # Convert each frame to HSV and modify the hue channel\n",
    "    adjusted_frames = []\n",
    "    hue_shift = random.randint(-10, 10)  # Random hue shift\n",
    "    for frame in video_frames:\n",
    "        hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        hsv_frame[..., 0] = hsv_frame[..., 0] + hue_shift\n",
    "        hsv_frame[..., 0] = np.clip(hsv_frame[..., 0], 0, 179)  # Ensure hue stays in valid range\n",
    "        adjusted_frame = cv2.cvtColor(hsv_frame, cv2.COLOR_HSV2BGR)\n",
    "        adjusted_frames.append(adjusted_frame)\n",
    "    return adjusted_frames\n",
    "\n",
    "def crop_video(video_frames, crop_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Crops the video frames by a random crop ratio (default: 80%).\n",
    "    \"\"\"\n",
    "    height, width, _ = video_frames[0].shape\n",
    "    crop_width = int(width * crop_ratio)\n",
    "    crop_height = int(height * crop_ratio)\n",
    "\n",
    "    # Randomly select crop position (center crop for simplicity)\n",
    "    start_x = random.randint(0, width - crop_width)\n",
    "    start_y = random.randint(0, height - crop_height)\n",
    "\n",
    "    cropped_frames = [frame[start_y:start_y + crop_height, start_x:start_x + crop_width] for frame in video_frames]\n",
    "    return cropped_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IC_dqjPwt067"
   },
   "outputs": [],
   "source": [
    "def interpolate_frames(video_frames, target_length):\n",
    "    \"\"\"\n",
    "    This function interpolates between the last few frames to create a smooth transition.\n",
    "    It will generate intermediate frames if the video is too short after augmentation.\n",
    "    \"\"\"\n",
    "    total_frames = len(video_frames)\n",
    "    if total_frames >= target_length:\n",
    "        return video_frames[:target_length]\n",
    "\n",
    "    padding_needed = target_length - total_frames\n",
    "    interpolated_frames = video_frames.copy()\n",
    "\n",
    "    # Interpolate between the last 2 frames to generate intermediate frames\n",
    "    for i in range(padding_needed):\n",
    "        t = (i + 1) / (padding_needed + 1)  # Calculate interpolation factor\n",
    "        frame1 = video_frames[-2]  # Second last frame\n",
    "        frame2 = video_frames[-1]  # Last frame\n",
    "\n",
    "        # Perform linear interpolation for each channel\n",
    "        interpolated_frame = cv2.addWeighted(frame1, 1 - t, frame2, t, 0)\n",
    "        interpolated_frames.append(interpolated_frame)\n",
    "\n",
    "    return interpolated_frames\n",
    "\n",
    "\n",
    "def apply_fixed_temporal_length(video_frames, target_length=120, original_fps=30, target_fps=12):\n",
    "    \"\"\"\n",
    "    This function ensures that the video frames have a fixed temporal length by resizing the video frames.\n",
    "    Downsamples the video to target_fps if necessary.\n",
    "    \"\"\"\n",
    "    total_frames = len(video_frames)\n",
    "\n",
    "    # If the video has more frames than target_length, downsample to the target length\n",
    "    if total_frames > target_length:\n",
    "        # Calculate the frame indices to sample based on target FPS\n",
    "        indices = np.linspace(0, total_frames - 1, target_length).astype(int)\n",
    "        video_frames = [video_frames[i] for i in indices]\n",
    "    elif total_frames < target_length:\n",
    "        # If the video has fewer frames than target, interpolate\n",
    "        video_frames = interpolate_frames(video_frames, target_length)\n",
    "\n",
    "    return video_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgLkgjMKU8FT"
   },
   "outputs": [],
   "source": [
    "def apply_augmentation_to_video(video_frames, target_length=120):\n",
    "    \"\"\"\n",
    "    This function applies a random combination of 2 to 5 augmentations to the entire video.\n",
    "    \"\"\"\n",
    "    augmentations = ['rotate', 'flip', 'brightness', 'contrast', 'saturation', 'hue', 'crop', 'speed']\n",
    "\n",
    "    # Randomly select a number of augmentations (2 to 5)\n",
    "    num_augmentations = random.randint(2, 5)\n",
    "    selected_augmentations = random.sample(augmentations, num_augmentations)\n",
    "\n",
    "    # Apply the selected augmentations\n",
    "    for augmentation in selected_augmentations:\n",
    "        if augmentation == 'rotate':\n",
    "            angle = random.randint(-30, 30)\n",
    "            video_frames = rotate_video(video_frames, angle)\n",
    "        elif augmentation == 'flip':\n",
    "            video_frames = flip_video(video_frames)\n",
    "        elif augmentation == 'brightness':\n",
    "            video_frames = adjust_brightness(video_frames)\n",
    "        elif augmentation == 'contrast':\n",
    "            video_frames = adjust_contrast(video_frames)\n",
    "        elif augmentation == 'saturation':\n",
    "            video_frames = adjust_saturation(video_frames)\n",
    "        elif augmentation == 'hue':\n",
    "            video_frames = adjust_hue(video_frames)\n",
    "        elif augmentation == 'crop':\n",
    "            video_frames = crop_video(video_frames, crop_ratio=random.uniform(0.7, 1.0))  # Random crop ratio between 70% to 100%\n",
    "        elif augmentation == 'speed':\n",
    "            speed_factor = random.uniform(0.5, 1.5)  # Speed change between 50% and 150%\n",
    "            video_frames = change_speed(video_frames, speed_factor)\n",
    "\n",
    "    # Apply fixed temporal length (truncate or pad)\n",
    "    video_frames = apply_fixed_temporal_length(video_frames, target_length)\n",
    "\n",
    "    return video_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9btv5GFiBjK5"
   },
   "outputs": [],
   "source": [
    "def augment_video(input_video_path, output_video_path, target_fps=12, target_length=120):\n",
    "    \"\"\"\n",
    "    Augments a video by applying a random combination of augmentations to the entire video.\n",
    "    Ensures the final video is exactly target_length frames long at target_fps.\n",
    "    \"\"\"\n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Apply random augmentations\n",
    "    augmented_frames = apply_augmentation_to_video(frames)\n",
    "\n",
    "    # Ensure the video has the fixed temporal length (120 frames at 12 fps)\n",
    "    augmented_frames = apply_fixed_temporal_length(augmented_frames, target_length, original_fps, target_fps)\n",
    "\n",
    "    # Ensure the output dimensions match the input dimensions\n",
    "    augmented_frames = [cv2.resize(frame, (frame_width, frame_height)) for frame in augmented_frames]\n",
    "\n",
    "    # Write the augmented video to the output file\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, target_fps, (frame_width, frame_height))\n",
    "\n",
    "    for frame in augmented_frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Augmented video saved to {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1739293790200,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "BhM_p8rgVj5r",
    "outputId": "24791bff-dfec-43ef-ff16-f074f4885a23"
   },
   "outputs": [],
   "source": [
    "# Path to the train folder\n",
    "train_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/final_splits/train/\"\n",
    "\n",
    "# Subdirectories for each class\n",
    "fall_folder = os.path.join(train_folder, 'fall')\n",
    "no_fall_folder = os.path.join(train_folder, 'no_fall')\n",
    "\n",
    "# Get all video files from both classes (fall and no_fall)\n",
    "fall_videos = [os.path.join(fall_folder, video) for video in os.listdir(fall_folder) if video.endswith('.mp4')]\n",
    "no_fall_videos = [os.path.join(no_fall_folder, video) for video in os.listdir(no_fall_folder) if video.endswith('.mp4')]\n",
    "\n",
    "# Combine the lists for easy processing\n",
    "all_videos = fall_videos + no_fall_videos\n",
    "\n",
    "# Print out the loaded video paths to check\n",
    "print(\"Fall Videos:\")\n",
    "for video in fall_videos:\n",
    "    print(video)\n",
    "\n",
    "print(\"\\nNo Fall Videos:\")\n",
    "for video in no_fall_videos:\n",
    "    print(video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22543,
     "status": "ok",
     "timestamp": 1739293829306,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "5DlBRPaV2-Yf",
    "outputId": "abb8a343-003c-4a6f-a993-c27b42acfb2a"
   },
   "outputs": [],
   "source": [
    "video_analysis(fall_videos, no_fall_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739293831800,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "QAEeX4VAvzYW",
    "outputId": "2bff34be-7607-44bb-e6cc-ac32aad48644"
   },
   "outputs": [],
   "source": [
    "# Number of augmented videos we want to create\n",
    "num_augmented_videos = 30  # For fall videos\n",
    "\n",
    "# Loop over the fall videos and augment them\n",
    "augmented_fall_count = 0\n",
    "for video_path in fall_videos:\n",
    "    # Augment each video and save them with a new name\n",
    "    for i in range(num_augmented_videos // len(fall_videos)):  # Divide the total augmentations across videos\n",
    "        video_name = video_path.split('/')[-1].split('.')[0]\n",
    "        output_video_path = f\"{fall_folder}/{video_name}-augmented-{augmented_fall_count+1}.mp4\"\n",
    "\n",
    "        # Augment the video\n",
    "        augment_video(video_path, output_video_path)\n",
    "\n",
    "        augmented_fall_count += 1\n",
    "\n",
    "print(\"Fall videos successfully augmented!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739293833788,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "es4bXjyFICek",
    "outputId": "c0f9fe3d-b51f-485c-9f58-4ad336e92d47"
   },
   "outputs": [],
   "source": [
    "# Number of augmented videos we want to create\n",
    "num_augmented_videos = 25  # For no_fall videos\n",
    "\n",
    "# Loop over the no_fall videos and augment them\n",
    "augmented_no_fall_count = 0\n",
    "for video_path in no_fall_videos:\n",
    "    # Augment each video and save them with a new name\n",
    "    for i in range(num_augmented_videos // len(no_fall_videos)):  # Divide the total augmentations across videos\n",
    "        video_name = video_path.split('/')[-1].split('.')[0]\n",
    "        output_video_path = f\"{no_fall_folder}/{video_name}-augmented-{augmented_no_fall_count+1}.mp4\"\n",
    "\n",
    "        # Augment the video\n",
    "        augment_video(video_path, output_video_path)\n",
    "\n",
    "        augmented_no_fall_count += 1\n",
    "\n",
    "print(\"No-fall videos successfully augmented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739293836941,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "tgFob7V5KszS",
    "outputId": "e4370182-b309-4be7-b6ab-7472ec039c77"
   },
   "outputs": [],
   "source": [
    "# Get all video files from both classes (fall and no_fall)\n",
    "fall_videos = [os.path.join(fall_folder, video) for video in os.listdir(fall_folder) if video.endswith('.mp4')]\n",
    "no_fall_videos = [os.path.join(no_fall_folder, video) for video in os.listdir(no_fall_folder) if video.endswith('.mp4')]\n",
    "\n",
    "print(len(fall_videos), len(no_fall_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "3iX8Kleiv_jd"
   },
   "outputs": [],
   "source": [
    "# Repititive work [To fix later]: Processing the whole folder once again to ensure 120 frames (10 seconds length) in each video\n",
    "\n",
    "\n",
    "def process_videos_to_fixed_length(video_paths, target_frames=120, target_fps=12):\n",
    "    for video_path in video_paths:\n",
    "        # Open the video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Get video properties\n",
    "        original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        # Extract frames from the video\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Downsample from 30 fps to 12 fps if the original fps is 30\n",
    "        if original_fps == 30:\n",
    "            downsampled_frames = []\n",
    "            frame_interval = int(original_fps / target_fps)  # Interval for downsampling (30/12 = 2.5, round to 2)\n",
    "\n",
    "            for i in range(0, len(frames), frame_interval):\n",
    "                downsampled_frames.append(frames[i])\n",
    "\n",
    "            frames = downsampled_frames  # Replace frames with downsampled frames\n",
    "\n",
    "        # Ensure the video has the target number of frames (120)\n",
    "        total_frames = len(frames)\n",
    "\n",
    "        if total_frames < target_frames:\n",
    "            # If the video is shorter than 120 frames, pad by repeating the last frame\n",
    "            frames.extend([frames[-1]] * (target_frames - total_frames))\n",
    "        elif total_frames > target_frames:\n",
    "            # If the video is longer than 120 frames, truncate\n",
    "            frames = frames[:target_frames]\n",
    "\n",
    "        # Overwrite the original video with the processed one at 12 fps\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(video_path, fourcc, target_fps, (frame_width, frame_height))  # Save at 12 FPS for final video\n",
    "\n",
    "        for frame in frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "        print(f\"Processed video: {video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrkdlJk9Sc4z"
   },
   "outputs": [],
   "source": [
    "# Paths to the train folder\n",
    "train_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/final_splits/train/\"\n",
    "fall_folder = os.path.join(train_folder, 'fall')\n",
    "no_fall_folder = os.path.join(train_folder, 'no_fall')\n",
    "\n",
    "# Get all video files from both classes\n",
    "fall_videos = [os.path.join(fall_folder, video) for video in os.listdir(fall_folder) if video.endswith('.mp4')]\n",
    "no_fall_videos = [os.path.join(no_fall_folder, video) for video in os.listdir(no_fall_folder) if video.endswith('.mp4')]\n",
    "\n",
    "# Combine video paths for processing\n",
    "all_videos = fall_videos + no_fall_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7110,
     "status": "ok",
     "timestamp": 1739293853161,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "Y13Ay0GxSfRs",
    "outputId": "3421b7b3-2567-4f6d-e96e-aae9574fc8a3"
   },
   "outputs": [],
   "source": [
    "# Process all videos to ensure fixed temporal length\n",
    "print(\"Processing all videos to fixed temporal length...\")\n",
    "process_videos_to_fixed_length(all_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739293858846,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "Clp62srbxebR",
    "outputId": "9fec930d-530e-45f8-c5d1-080946e42aa4"
   },
   "outputs": [],
   "source": [
    "print(len(fall_videos), len(no_fall_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1739293862148,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "hGn7sN7cYYQz",
    "outputId": "4637ce43-037a-40ec-9c99-f19d3824e2dd"
   },
   "outputs": [],
   "source": [
    "video_analysis(fall_videos, no_fall_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Y9s3Tgy-TAu"
   },
   "source": [
    "###Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zh3ClsAV-Xa5"
   },
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_size=(160, 160), max_frames=120):\n",
    "    \"\"\"\n",
    "    Extract frames from a video file, resize them, normalize, and ensure the total number of frames is capped.\n",
    "    Repeats the last frame if the number of frames is less than max_frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "        # Resize and normalize the frame\n",
    "        frame = cv2.resize(frame, frame_size) / 255.0\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # If video has fewer than max_frames, repeat the last frame\n",
    "    if len(frames) < max_frames:\n",
    "        last_frame = frames[-1]\n",
    "        frames.extend([last_frame] * (max_frames - len(frames)))  # Repeat the last frame\n",
    "\n",
    "    # Truncate if it exceeds the max_frames limit\n",
    "    return np.array(frames[:max_frames])  # Ensure it never exceeds max_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6K0xBLd-udN"
   },
   "outputs": [],
   "source": [
    "def preprocess_video_data(video_paths, batch_size, output_folder, frame_size=(160, 160), max_frames=120):\n",
    "    \"\"\"\n",
    "    Process videos in batches, extract frames, save the processed data and labels to disk in compressed .npz format.\n",
    "    \"\"\"\n",
    "    num_videos = len(video_paths)\n",
    "    num_batches = (num_videos + batch_size - 1) // batch_size  # Ceiling division\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        print(f\"Processing batch {batch_idx + 1} of {num_batches}...\")\n",
    "\n",
    "        batch_videos = video_paths[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for video_path in batch_videos:\n",
    "            # Extract frames from the video\n",
    "            frames = extract_frames(video_path, frame_size, max_frames)\n",
    "\n",
    "            # Append to batch\n",
    "            batch_data.append(frames)\n",
    "            # Assign label (1 for 'fall', 0 for 'no_fall')\n",
    "            label = 0 if 'no_fall' in video_path else 1\n",
    "            batch_labels.append(label)\n",
    "\n",
    "        # Convert batch to numpy arrays\n",
    "        batch_data = np.array(batch_data, dtype=np.float32)\n",
    "        batch_labels = np.array(batch_labels, dtype=np.int32)\n",
    "\n",
    "        # Define compressed file paths\n",
    "        batch_data_file = os.path.join(output_folder, f'batch_{batch_idx}_data.npz')\n",
    "        batch_labels_file = os.path.join(output_folder, f'batch_{batch_idx}_labels.npz')\n",
    "\n",
    "        # Save compressed files (using npz format for compression)\n",
    "        np.savez_compressed(batch_data_file, batch_data)\n",
    "        np.savez_compressed(batch_labels_file, batch_labels)\n",
    "\n",
    "        print(f\"Batch {batch_idx + 1} saved to disk.\")\n",
    "\n",
    "    print(\"Processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1739293870633,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "AgK96K6_A5nH",
    "outputId": "3a722143-8e35-41a4-953a-a9e0633e2918"
   },
   "outputs": [],
   "source": [
    "# Directories for fall and no_fall videos in train, validation, and test\n",
    "train_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/final_splits/train/\"\n",
    "val_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/final_splits/val/\"\n",
    "test_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/final_splits/test/\"\n",
    "\n",
    "# Output folders for batches\n",
    "train_output_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/processed_batches/train/\"\n",
    "val_output_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/processed_batches/val/\"\n",
    "test_output_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/processed_batches/test/\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(train_output_folder, exist_ok=True)\n",
    "os.makedirs(val_output_folder, exist_ok=True)\n",
    "os.makedirs(test_output_folder, exist_ok=True)\n",
    "\n",
    "# Get all video files from each class in the train, validation, and test sets\n",
    "train_fall_videos = [os.path.join(train_folder, 'fall', video) for video in os.listdir(os.path.join(train_folder, 'fall')) if video.endswith('.mp4')]\n",
    "train_no_fall_videos = [os.path.join(train_folder, 'no_fall', video) for video in os.listdir(os.path.join(train_folder, 'no_fall')) if video.endswith('.mp4')]\n",
    "\n",
    "val_fall_videos = [os.path.join(val_folder, 'fall', video) for video in os.listdir(os.path.join(val_folder, 'fall')) if video.endswith('.mp4')]\n",
    "val_no_fall_videos = [os.path.join(val_folder, 'no_fall', video) for video in os.listdir(os.path.join(val_folder, 'no_fall')) if video.endswith('.mp4')]\n",
    "\n",
    "test_fall_videos = [os.path.join(test_folder, 'fall', video) for video in os.listdir(os.path.join(test_folder, 'fall')) if video.endswith('.mp4')]\n",
    "test_no_fall_videos = [os.path.join(test_folder, 'no_fall', video) for video in os.listdir(os.path.join(test_folder, 'no_fall')) if video.endswith('.mp4')]\n",
    "\n",
    "import random\n",
    "\n",
    "# Shuffle individual class lists\n",
    "random.shuffle(train_fall_videos)\n",
    "random.shuffle(train_no_fall_videos)\n",
    "\n",
    "# Combine the shuffled class lists\n",
    "train_videos = train_fall_videos + train_no_fall_videos\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(train_videos)\n",
    "\n",
    "# Do the same for validation and test sets\n",
    "random.shuffle(val_fall_videos)\n",
    "random.shuffle(val_no_fall_videos)\n",
    "val_videos = val_fall_videos + val_no_fall_videos\n",
    "random.shuffle(val_videos)\n",
    "\n",
    "random.shuffle(test_fall_videos)\n",
    "random.shuffle(test_no_fall_videos)\n",
    "test_videos = test_fall_videos + test_no_fall_videos\n",
    "random.shuffle(test_videos)\n",
    "\n",
    "\n",
    "# Print counts of videos\n",
    "print(\"Number of videos:\")\n",
    "print(f\"Train: {len(train_videos)}, Validation: {len(val_videos)}, Test: {len(test_videos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269421,
     "status": "ok",
     "timestamp": 1739294143541,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "5IDxHzlqByfg",
    "outputId": "48693cab-40af-44c5-b35e-51efe3527cf4"
   },
   "outputs": [],
   "source": [
    "# Define the batch size as 5 videos per batch\n",
    "batch_size = 5\n",
    "\n",
    "# Process the data for each set in batches\n",
    "preprocess_video_data(train_videos, batch_size, train_output_folder)\n",
    "preprocess_video_data(val_videos, batch_size, val_output_folder)\n",
    "preprocess_video_data(test_videos, batch_size, test_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViuIJ8AA4BRk"
   },
   "source": [
    "###Working With Batch Data & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAKOLI-5wbNu"
   },
   "outputs": [],
   "source": [
    "def load_single_batch(data_file_path, labels_file_path):\n",
    "    \"\"\"\n",
    "    Load a single batch from the npz data and labels files.\n",
    "    :param data_file_path: Path to the batch data .npz file\n",
    "    :param labels_file_path: Path to the batch labels .npz file\n",
    "    :return: (batch_data, batch_labels)\n",
    "    \"\"\"\n",
    "    batch_data = np.load(data_file_path)['arr_0']\n",
    "    batch_labels = np.load(labels_file_path)['arr_0']\n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jALmE2PY547y"
   },
   "outputs": [],
   "source": [
    "def load_batches_one_at_a_time(batch_folder):\n",
    "    \"\"\"\n",
    "    Load batches from the given folder one batch at a time.\n",
    "    :param batch_folder: Path to the folder containing the batch files\n",
    "    :return: A generator yielding a single batch of data and labels at a time\n",
    "    \"\"\"\n",
    "    # List all npz files in the directory (for data and labels)\n",
    "    data_files = [file for file in os.listdir(batch_folder) if file.endswith('_data.npz')]\n",
    "    labels_files = [file for file in os.listdir(batch_folder) if file.endswith('_labels.npz')]\n",
    "\n",
    "    # Sort the files to ensure correct batch order\n",
    "    data_files.sort()\n",
    "    labels_files.sort()\n",
    "\n",
    "    assert len(data_files) == len(labels_files), \"Mismatch in number of data and label files\"\n",
    "\n",
    "    # Load one batch at a time\n",
    "    for i in range(len(data_files)):\n",
    "        data_file_path = os.path.join(batch_folder, data_files[i])\n",
    "        labels_file_path = os.path.join(batch_folder, labels_files[i])\n",
    "\n",
    "        # Load data and labels for this batch\n",
    "        batch_data, batch_labels = load_single_batch(data_file_path, labels_file_path)\n",
    "\n",
    "        # Yield a single batch of data and labels\n",
    "        yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2812,
     "status": "ok",
     "timestamp": 1739294416723,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "jLTjGYy37vaM",
    "outputId": "d7a9dbb0-8943-4f58-9831-d656a2823c8d"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cpj5YngK8TeK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.metrics\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnyvnJTMIhU9"
   },
   "outputs": [],
   "source": [
    "# Learning rate decay function\n",
    "def lr_decay(epoch, lr):\n",
    "    # Decaying the learning rate by a factor of 0.9 every 5 epochs\n",
    "    decay_factor = 0.9\n",
    "    drop_every = 5  # Drop learning rate every 5 epochs\n",
    "    if epoch % drop_every == 0 and epoch > 0:\n",
    "        return lr * decay_factor\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 3146,
     "status": "ok",
     "timestamp": 1739294524783,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "t5g-cWsV7sL9",
    "outputId": "44a8e31f-2241-473f-ceae-f8a7e7831e4e"
   },
   "outputs": [],
   "source": [
    "# Create model function (same as before)\n",
    "def create_model(input_shape=(120, 160, 160, 3)):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Feature extraction with 2D convolutions\n",
    "    model.add(layers.TimeDistributed(layers.Conv2D(16, (3, 3), activation='relu', padding='same'), input_shape=input_shape))\n",
    "    model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
    "    model.add(layers.TimeDistributed(layers.Conv2D(32, (3, 3), activation='relu', padding='same')))\n",
    "    model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
    "\n",
    "    # Flatten spatial dimensions\n",
    "    model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "\n",
    "    # Temporal modeling\n",
    "    model.add(layers.LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "    # Compile the model with Adam optimizer and a custom learning rate scheduler\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),  # initial learning rate\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "#  Create the model\n",
    "model = create_model(input_shape=(120, 160, 160, 3))\n",
    "\n",
    "# Summarize the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUV4GQON-GDc"
   },
   "outputs": [],
   "source": [
    "train_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/processed_batches/train/\"\n",
    "val_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/processed_batches/val/\"\n",
    "test_folder = \"/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/processed_batches/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 102192,
     "status": "error",
     "timestamp": 1739294950173,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "gv6LfCwGC4Sn",
    "outputId": "7157b952-3622-4955-caf9-dbea6ba49f4c"
   },
   "outputs": [],
   "source": [
    "# Define the training loop with learning rate scheduler callback\n",
    "epochs = 10\n",
    "\n",
    "# Setup the Learning Rate Scheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decay)\n",
    "\n",
    "# Training loop (without validation during training)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Training loop with batch generator\n",
    "    train_batch_generator = load_batches_one_at_a_time(train_folder)\n",
    "\n",
    "    for i, (batch_data, batch_labels) in enumerate(train_batch_generator):\n",
    "        batch_data = tf.convert_to_tensor(batch_data)\n",
    "        batch_labels = tf.convert_to_tensor(batch_labels)\n",
    "\n",
    "        # Train on the batch\n",
    "        metrics = model.train_on_batch(batch_data, batch_labels)\n",
    "\n",
    "        # Unpack metrics dynamically\n",
    "        metric_names = model.metrics_names  # Get metric names dynamically\n",
    "        metrics_dict = dict(zip(metric_names, metrics))\n",
    "\n",
    "        # Print metrics for training\n",
    "        print(f\"Batch {i + 1}: Training metrics: {metrics_dict}\")\n",
    "\n",
    "    # Call the learning rate scheduler at the end of each epoch\n",
    "    current_lr = lr_decay(epoch, model.optimizer.lr)\n",
    "    model.optimizer.lr.assign(current_lr)\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caW4yWvoGIvb"
   },
   "source": [
    "###Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7136,
     "status": "ok",
     "timestamp": 1733156823707,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "7zKGds6IHlxR",
    "outputId": "e3343172-fa0e-4707-f622-dc546594b2d1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Validation after training\n",
    "val_batch_generator = load_batches_one_at_a_time(val_folder)\n",
    "\n",
    "val_metrics = []\n",
    "for i, (val_data, val_labels) in enumerate(val_batch_generator):\n",
    "    val_data = tf.convert_to_tensor(val_data)\n",
    "    val_labels = tf.convert_to_tensor(val_labels)\n",
    "\n",
    "    # Validation metrics\n",
    "    val_metrics_batch = model.test_on_batch(val_data, val_labels)\n",
    "    val_metrics.append(val_metrics_batch)\n",
    "\n",
    "    y_pred = model.predict(val_data)\n",
    "    cm = confusion_matrix(val_labels, y_pred > 0.5)\n",
    "    print(cm)\n",
    "\n",
    "# Calculate average validation metrics for the entire validation set\n",
    "avg_val_metrics = np.mean(val_metrics, axis=0)\n",
    "val_metrics_dict = dict(zip(model.metrics_names, avg_val_metrics))\n",
    "\n",
    "print(f\"Validation metrics after training: {val_metrics_dict}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EcABrrcPTef"
   },
   "source": [
    "###Tesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9792,
     "status": "ok",
     "timestamp": 1733156836207,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "S_maEb9DPPtX",
    "outputId": "4233e811-b348-4a68-d631-6d92446ea6b4"
   },
   "outputs": [],
   "source": [
    "# Test the model on the test set\n",
    "\n",
    "test_batch_generator = load_batches_one_at_a_time(test_folder)\n",
    "\n",
    "test_metrics = []\n",
    "for i, (test_data, test_labels) in enumerate(test_batch_generator):\n",
    "    test_data = tf.convert_to_tensor(test_data)\n",
    "    test_labels = tf.convert_to_tensor(test_labels)\n",
    "\n",
    "    # Test metrics\n",
    "    test_metrics_batch = model.test_on_batch(test_data, test_labels)\n",
    "    test_metrics.append(test_metrics_batch)\n",
    "\n",
    "    y_pred = model.predict(test_data)\n",
    "    cm = confusion_matrix(test_labels, y_pred > 0.5)\n",
    "    print(cm)\n",
    "\n",
    "# Calculate average test metrics\n",
    "avg_test_metrics = np.mean(test_metrics, axis=0)\n",
    "test_metrics_dict = dict(zip(model.metrics_names, avg_test_metrics))\n",
    "\n",
    "print(f\"Test metrics: {test_metrics_dict}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOXwwq0nyatx"
   },
   "source": [
    "###Testing on External Video(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3876,
     "status": "ok",
     "timestamp": 1733156865568,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "xgM9J4aUye5R",
    "outputId": "9ac31a95-d93a-4b1a-d521-04c9dd17fbe2"
   },
   "outputs": [],
   "source": [
    "# Step 1: Process the video to 12 fps, 120 frames, and resize to 160x160\n",
    "def process_video(video_path, fps=12, target_length=120, target_size=(160, 160)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_interval = int(original_fps // fps)\n",
    "    selected_frames = []\n",
    "\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_interval == 0 and len(selected_frames) < target_length:\n",
    "            frame_resized = cv2.resize(frame, target_size)\n",
    "            selected_frames.append(frame_resized)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        if len(selected_frames) >= target_length:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(selected_frames)\n",
    "\n",
    "# Process the video\n",
    "path = '/content/drive/MyDrive/Fall_Detection_Playground_Senior_Prjct_II/external_test_vids/'\n",
    "video_path = ['fall.mp4']\n",
    "\n",
    "for videos in video_path:\n",
    "  v_path = path + videos\n",
    "  processed_video = process_video(v_path)\n",
    "  # Ensure the video has shape (1, 120, 160, 160, 3) as your model expects\n",
    "  processed_video_tensor = tf.convert_to_tensor(processed_video[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "  # Step 3: Predict using the model\n",
    "  predictions = model.predict(processed_video_tensor)\n",
    "\n",
    "  # Step 4: Convert prediction to class label (e.g., 0 for 'no_fall', 1 for 'fall')\n",
    "  # predicted_class = \"fall\" if predictions[0][0] > 0.5 else \"no_fall\"\n",
    "  # print(f\"The video is predicted as: {predicted_class}, {predictions[0][0]}\")\n",
    "  # print(videos, predictions[0][0])\n",
    "\n",
    "  # , 'test_video_1.mp4', 'test.mp4', 'test_video.mp4', 'test_video_2.mp4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2076,
     "status": "ok",
     "timestamp": 1733151514562,
     "user": {
      "displayName": "Prasun Dhungana",
      "userId": "15869599958843435384"
     },
     "user_tz": 300
    },
    "id": "D_cGePIOzEBB",
    "outputId": "c4a8440b-c437-4b16-8c66-84891c6a3e8d"
   },
   "outputs": [],
   "source": [
    "# Ensure the video has shape (1, 120, 160, 160, 3) as your model expects\n",
    "processed_video_tensor = tf.convert_to_tensor(processed_video[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "# Step 3: Predict using the model\n",
    "predictions = model.predict(processed_video_tensor)\n",
    "\n",
    "# Step 4: Convert prediction to class label (e.g., 0 for 'no_fall', 1 for 'fall')\n",
    "# predicted_class = \"fall\" if predictions[0][0] > 0.5 else \"no_fall\"\n",
    "# print(f\"The video is predicted as: {predicted_class}, {predictions[0][0]}\")\n",
    "print(predictions[0][0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
